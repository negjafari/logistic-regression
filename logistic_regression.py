# -*- coding: utf-8 -*-
"""logistic regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rXYA2Ueddu3fh4y5TXucHxm5n64-9cQA

Import essential libraries
"""

from google.colab import drive
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

drive.mount('/content/drive')

dataset_path1 = '/content/drive/MyDrive/logistic regression dataset/breast_cancer_modified.csv'
df1 = pd.read_csv(dataset_path1)

dataset_path2 = '/content/drive/MyDrive/logistic regression dataset/ionosphere_modified.csv'
df2 = pd.read_csv(dataset_path2)

df1_original = pd.read_csv(dataset_path1)

"""# Dataset Analysis"""

pd.set_option('display.max_columns', None)  # Set to None to display all columns

def analyzeDataset(df):
    missing_values = df.isnull().sum()
    fig, ax = plt.subplots(figsize=(12, 6))
    missing_values.plot.bar(ax=ax)
    for i, val in enumerate(missing_values):
        ax.text(i, val + 10, str(val), ha='center', fontweight='bold')

    plt.show()

analyzeDataset(df1)

analyzeDataset(df2)

df1.describe()

df2.describe()

# Distribution of classes
print(df1['Diagnosis'].value_counts())
print(df2['Label'].value_counts())

# Class distribution for Breast Cancer dataset
sns.countplot(x='Diagnosis', data=df1)
plt.title('Distribution of Classes in Breast Cancer Dataset')
plt.show()

# Class distribution for Ionosphere dataset
sns.countplot(x='Label', data=df2)
plt.title('Distribution of Classes in Ionosphere Dataset')
plt.show()

# Feature distributions in Breast Cancer dataset
features_bc = df1.columns[1:]
for feature in features_bc:
    sns.histplot(df1[feature], kde=True)
    plt.title(f'Distribution of {feature} - Breast Cancer')
    plt.show()

# Feature distributions in Ionosphere dataset
features_io = df2.columns[:]
for feature in features_io:
    sns.histplot(df2[feature], kde=True)
    plt.title(f'Distribution of {feature} - Ionosphere')
    plt.show()

df1 = df1.drop(columns=['ID'])

def visualize_correlation_matrix(df, title):
    corr_matrix = df.corr()
    # Mask to hide upper triangle
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

    # Setting up the matplotlib figure
    f, ax = plt.subplots(figsize=(11, 9))

    # Generating a custom diverging colormap
    cmap = sns.diverging_palette(230, 20, as_cmap=True)

    # Draw the heatmap with the mask and correct aspect ratio
    sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,
                square=True, linewidths=.5, cbar_kws={"shrink": .5},
                annot=False)  # Annotations are turned off for clarity
    plt.title(title)
    plt.show()

visualize_correlation_matrix(df1, 'Full Correlation Matrix for Breast Cancer Dataset')

visualize_correlation_matrix(df2, 'Full Correlation Matrix for Ionosphere Dataset')

"""# Preprocessing

Encode categorical variables
"""

df1['Diagnosis'] = df1['Diagnosis'].map({'B': 1, 'M': 0})
df2['Label'] = df2['Label'].map({'g': 1, 'b': 0})

df1.describe()

# select all features except the target
features_bc = df1.columns[:-1]
features_io = df2.columns[:-1]

# Then apply the manual standardization as shown before
for column in features_bc:
    df1[column] = (df1[column] - df1[column].mean()) / df1[column].std()

for column in features_io:
    df2[column] = (df2[column] - df2[column].mean()) / df2[column].std()

# Assuming 'feature1' is the feature you're interested in
feature = features_bc[0]  # This should be adjusted to your feature of interest's actual name

# Plot before standardization
sns.histplot(df1_original[feature], color="blue", label="Before Standardization", kde=True)

# Plot after standardization
sns.histplot(df1[feature], color="red", label="After Standardization", kde=True)

plt.legend()
plt.title(f'Distribution of {feature} Before and After Standardization')
plt.xlabel(f'{feature} Value')
plt.ylabel('Frequency')
plt.show()

# Data for plotting
data_to_plot = [df1_original[feature], df1[feature]]

# Creating the box plot
plt.boxplot(data_to_plot, patch_artist=True, labels=['Before', 'After'])

plt.title(f'Box Plot of {feature} Before and After Standardization')
plt.ylabel(f'{feature} Value')
plt.xticks([1, 2], ['Before Standardization', 'After Standardization'])
plt.show()

def normalize_features(df, feature_names):
    for feature in feature_names:
        min_value = df[feature].min()
        max_value = df[feature].max()
        df[feature] = (df[feature] - min_value) / (max_value - min_value)

# Assuming features_bc and features_io are lists of your feature column names
normalize_features(df1, features_bc)
normalize_features(df2, features_io)

"""# Logistic Regression"""

import numpy as np

def train_test_split(X, y, test_size=0.2, random_state=None):
    if random_state:
        np.random.seed(random_state)

    # Concatenate X and y to shuffle them together
    full_dataset = np.concatenate((X, y.reshape(-1, 1)), axis=1)
    np.random.shuffle(full_dataset)

    # Calculate the number of training examples
    train_size = int(full_dataset.shape[0] * (1 - test_size))

    # Split the dataset
    train, test = full_dataset[:train_size], full_dataset[train_size:]

    # Split back into X and y
    X_train = train[:, :-1]
    y_train = train[:, -1]
    X_test = test[:, :-1]
    y_test = test[:, -1]

    return X_train, X_test, y_train, y_test

X = df1.drop('Diagnosis', axis=1).values
y = df1['Diagnosis'].values

# Now you can call the train_test_split function
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X train shape : ', X_train.shape)
print('Y train shape : ', y_train.shape)
print('X test shape : ', X_test.shape)
print('Y test shape : ', y_test.shape)

class LogisticRegression:
    def __init__(self, learning_rate=0.01, iterations=1000):
        self.learning_rate = learning_rate
        self.iterations = iterations
        self.weights = None
        self.bias = None

    def _sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        # Gradient Descent
        for _ in range(self.iterations):
            model = np.dot(X, self.weights) + self.bias
            predictions = self._sigmoid(model)

            # Compute gradients
            dw = (1 / n_samples) * np.dot(X.T, (predictions - y))
            db = (1 / n_samples) * np.sum(predictions - y)

            # Update parameters
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

    def predict(self, X):
        model = np.dot(X, self.weights) + self.bias
        predictions = self._sigmoid(model)
        predicted_classes = [1 if i > 0.5 else 0 for i in predictions]
        return np.array(predicted_classes)

# Example of instantiating and training the logistic regression model
model = LogisticRegression(learning_rate=0.01, iterations=1000)
model.fit(X_train, y_train)

# Making predictions
predictions = model.predict(X_test)

def Accu_eval(y_true, y_pred):
    """
    Calculate the accuracy of the model.

    Parameters:
    - y_true: numpy.ndarray, the true labels.
    - y_pred: numpy.ndarray, the predicted labels by the model.

    Returns:
    - accuracy: float, the accuracy score of the model.
    """
    correct_predictions = (y_true == y_pred).sum()
    total_predictions = len(y_true)
    accuracy = correct_predictions / total_predictions
    return accuracy

# Assuming y_test are your true labels and predictions are what your model predicted
accuracy_score = Accu_eval(y_test, predictions)
print(f"Model Accuracy: {accuracy_score*100:.2f}%")

class KFoldCV:
    def __init__(self, model, k=10):
        self.model = model
        self.k = k
        self.scores = []

    def split(self, X, y):
        """Yield k equally sized splits of X and y using numpy arrays."""
        fold_size = len(X) // self.k
        indices = np.arange(len(X))
        np.random.shuffle(indices)

        for i in range(self.k):
            start = i * fold_size
            end = (i + 1) * fold_size if i != self.k - 1 else len(X)
            test_indices = indices[start:end]
            train_indices = np.concatenate([indices[:start], indices[end:]])

            X_train, X_test = X[train_indices], X[test_indices]
            y_train, y_test = y[train_indices], y[test_indices]
            yield X_train, X_test, y_train, y_test

    def fit_predict(self, X_train, X_test, y_train, y_test):
        self.model.fit(X_train, y_train)
        predictions = self.model.predict(X_test)
        return predictions

    def evaluate(self, y_true, y_pred):
        correct_predictions = (y_true == y_pred).sum()
        total_predictions = len(y_true)
        return correct_predictions / total_predictions

    def cross_validate(self, X, y):
        for X_train, X_test, y_train, y_test in self.split(X, y):
            y_pred = self.fit_predict(X_train, X_test, y_train, y_test)
            score = self.evaluate(y_test, y_pred)
            self.scores.append(score)
        return np.mean(self.scores), np.std(self.scores)

# Assuming LogisticRegression is your model class
model = LogisticRegression(learning_rate=0.01, iterations=1000)

X = df1.drop('Diagnosis', axis=1).values
y = df1['Diagnosis'].values

kf_cv = KFoldCV(model, k=10)
average_score, score_std = kf_cv.cross_validate(X, y)

print(f"Average Accuracy: {average_score*100:.2f}%")
# print(f"Standard Deviation of Accuracy: {score_std*100:.2f}%")

